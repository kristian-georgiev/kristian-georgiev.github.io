<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-113380303-1"
    ></script>

    <!-- Meta Tags -->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Kristian Georgiev</title>

    <!-- CSS  -->
    <link rel="shortcut icon" href="logos/logo.ico" type="image/x-icon" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1"
      crossorigin="anonymous"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&family=Roboto:wght@100;300;400;500;700;900&display=swap"
      rel="stylesheet"
    />
    <link
      href="css/style.css"
      type="text/css"
      rel="stylesheet"
      media="screen,projection"
    />
  </head>

  <!-- Home Section -->
  <body>
    <div class="container">
      <!-- about me section -->
      <div class="row">
        <div class="col-sm">
          <h1>Kristian Georgiev</h1>
          <h6>
            PhD candidate,
            <a href="https://www.eecs.mit.edu/" target="_blank">MIT EECS</a>
          </h6>
          <h6>S.B. MIT '21</h6>
          <p>
            I am a second-year PhD student advised by
            <a href="https://people.csail.mit.edu/madry/" target="_blank"
              >Aleksander Mądry</a
            >. I am interested in the
            <a href="https://gradientscience.org/about/" target="_blank">
              science of deep learning</a
            >
            and deep learning for science. I also like thinking about
            adversarial robustness and privacy.
          </p>

          <p>
            I obtained a joint degree in Math and Computer Science from
            MIT. During that time, I was fortunate to be advised by
            <a
              href="http://asu.mit.edu/"
              target="_blank"
              rel="noopener noreferrer"
              >Asuman Ozdaglar</a
            >
            as a
            <a
              href="https://superurop.mit.edu/scholars/kristian-georgiev/"
              target="_blank"
              rel="noopener noreferrer"
              >SuperUROP scholar</a
            >
            working on equivariance in deep learning. Previously, I was at
            <a href="https://quantco.com/" target="_blank">QuantCo</a> and
            <a href="https://www.apple.com/siri/" target="_blank">Apple</a>.
          </p>
          <p>
            I grew up in Bulgaria. In my free time, I enjoy playing soccer and
            chess. At MIT, I have led the
            <a href="http://aiclub.mit.edu/" target="_blank">AI@MIT</a> club.
          </p>

          <p>
            This website was last updated on
            <script>
              var date = new Date(document.lastModified);
              document.write(date.toDateString().split(" ").slice(1).join(" "));
            </script>.
          </p>
        </div>
        <div class="col-sm">
          <div class="card align-items-center" style="max-width: 95%">
            <img
              alt="Profile Photo Kristian Georgiev"
              src="img/profile_photo.jpg"
              class="rounded mx-auto d-block"
              style="max-width: 100%"
            />
            <div class="card-body align-items-center">
              <ul class="list-group list-group-flush">
                <li class="list-group-item text-center">krisgrg@mit.edu</li>
                <li class="list-group-item text-center">
                  <a
                    href="https://twitter.com/kris_georgiev1"
                    target="_blank"
                    rel="noopener noreferrer"
                    >Twitter</a
                  >
                  |
                  <a
                    href="https://scholar.google.com/citations?user=t8RKSJsAAAAJ&hl=en"
                    target="_blank"
                    rel="noopener noreferrer"
                    >Google Scholar</a
                  >
                  |
                  <a
                    href="https://github.com/kristian-georgiev/"
                    target="_blank"
                    rel="noopener noreferrer"
                    >GitHub</a
                  >
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <!-- Publications section -->
      <h2 style="margin-top: 40px">Manuscripts / Preprints</h2>
      <p>
        * denotes equal contribution; authors are ordered alphabetically unless
        they're not
      </p>
      <ul class="list-group list-group-flush">
        <li class="list-group-item">
          <strong> TRAK: Attributing Model Behavior at Scale </strong>
          <a
            href="https://arxiv.org/abs/2303.14186"
            target="_blank"
            rel="noopener noreferrer"
          >
            [arxiv]</a
          >
          <a
            href="http://gradientscience.org/trak/"
            target="_blank"
            rel="noopener noreferrer"
          >
            [blog post]</a
          >
          <a
            href="https://github.com/MadryLab/trak"
            target="_blank"
            rel="noopener noreferrer"
          >
            [python package]</a
          >
          <a
            href="https://trak.csail.mit.edu/"
            target="_blank"
            rel="noopener noreferrer"
          >
            [website]</a
          >

          <br />
          Sung Min Park*, Kristian Georgiev*, Andrew Ilyas*, Guillaume Leclerc,
          Aleksander Madry
          <br />
        </li>
      </ul>
      <h2 style="margin-top: 40px">Publications</h2>
      <ul class="list-group list-group-flush">
        <li class="list-group-item">
          <strong>
            Privacy Induces Robustness: Information-Computation Gaps and Sparse
            Mean Estimation
          </strong>
          <a
            href="https://arxiv.org/abs/2211.00724"
            target="_blank"
            rel="noopener noreferrer"
          >
            [arxiv]</a
          >
          <br />
          Kristian Georgiev, Samuel B. Hopkins
          <br />
          NeurIPS 2022
        </li>
        <li class="list-group-item">
          <strong> Implicit Bias of Linear Equivariant Networks </strong>
          <a
            href="https://arxiv.org/abs/2110.06084"
            target="_blank"
            rel="noopener noreferrer"
          >
            [arxiv]</a
          >
          <br />
          Hannah Lawrence, Kristian Georgiev, Andrew Dienes, Bobak T. Kiani
          <br />
          ICML 2022
        </li>
        <li class="list-group-item">
          <strong
            >On the Convergence Theory of Debiased Model-Agnostic
            Meta-Reinforcement Learning</strong
          >
          <a
            href="https://arxiv.org/abs/2002.05135"
            target="_blank"
            rel="noopener noreferrer"
          >
            [arxiv]
          </a>
          <br />
          Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, Asuman Ozdaglar
          <br />
          NeurIPS 2021
        </li>
      </ul>


      <!-- Projects section -->
      <h2 style="margin-top: 40px">
        Misc Projects
        <!-- <button type="button" class="btn btn-outline-danger toggle-all"> Expand all </button> -->
      </h2>
      a mix of class projects and stuff just for fun

      <div
        class="accordion accordion-flush"
        id="accordionFlushProjects"
        style="margin-top: 10px"
      >
        <!-- var coherent min max -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingVarCoh">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseVarCoh"
              aria-expanded="false"
              aria-controls="flush-collapseVarCoh"
              style="text-align: left"
            >
              Last Iterate Convergence of the Extragradient Method for
              Variationally Coherent Min-Max Problems (2020)
            </button>
          </h2>
          <div
            id="flush-collapseVarCoh"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingVarCoh"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1ZjVN4ljYuwom5x5FlNItjTcx7uikZIdv/view?usp=sharing"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              |
              <a
                href="https://drive.google.com/file/d/1dT_No7xE9xR2JNjKDFaKtkDrP0HexeC-/view?usp=sharing"
                target="_blank"
                rel="noopener noreferrer"
                >Slides</a
              >
              <figure class="figure">
                <img
                  src="img/var_coh.png"
                  class="figure-img img-fluid rounded"
                  alt="Variationally coherent paper figure"
                  style="max-width: 60%"
                />
                <figcaption class="figure-caption">
                  An example of a variationally coherent function [Zhou et al.,
                  2017]
                </figcaption>
              </figure>

              <p>
                In this work we analyze the behaviour of the last iterate of the
                Extragradient (EG) and Proximal Point (PP) algorithms on smooth
                variationally coherent problems1 . In particular, we examine a
                relaxation of the conditions in Golowich et al. (2020), who show
                a rate of \(\mathcal{O}\left( 1 / \sqrt{T} \right)\) for both
                algorithms when the problem is convex- concave. We show that
                best iterate convergence at a rate of \(\mathcal{O}\left( 1 /
                \sqrt{T} \right)\) naturally carries over to to variationally
                coherent problems but key monotonicity properties are lost.
              </p>
              <p class="text-muted">
                Golowich, N., Pattathil, S., Daskalakis, C., and Ozdaglar, A.
                Last iterate is slower than averaged iterate in smooth
                convex-concave saddle point problems, 2020.
              </p>
              <p class="text-muted">
                Zhou, Z., Mertikopoulos, P., Bambos, N., Boyd, S., and Glynn, P.
                W. Stochastic mirror descent in variationally coherent
                optimization problems, NeurIPS 2017.
              </p>
            </div>
          </div>
        </div>

        <!-- maml landscape -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingMeta">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseMeta"
              aria-expanded="false"
              aria-controls="flush-collapseMeta"
              style="text-align: left"
            >
              Meta-Visualization: Investigating Rapid Learning and Feature Reuse
              (2019)
            </button>
          </h2>
          <div
            id="flush-collapseMeta"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingMeta"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1gPAy1tuJ8uzb0Io1iFqrNalFb8Opkbpz/view?usp=sharing"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              |
              <a
                href="https://github.com/kristian-georgiev/867-Project"
                target="_blank"
                rel="noopener noreferrer"
                >Repo</a
              >
              <img
                src="img/meta_viz_fig.png"
                alt="meta visualization figure"
                style="max-width: 60%; margin-left: 20px"
              />
              <p>
                Gradient based meta-learning has established itself as a
                promising research direction for prob- lems in few-shot
                learning: data-constrained training on an ensemble of tasks with
                quick adapta- tion on a task, unseen during training. Until
                recently, little has been known about the success of
                model-agnostic meta-learners (MAMLs), which are particularly
                useful to few shot learning. We shed light on the phenomenon of
                feature reuse in MAMLs through empirical visualizations of the
                loss landscapes of a variety of tasks. We develop
                meta-visualization: an efficient framework for visualization of
                any gradient based meta learning algorithm that can be used to
                generate hypotheses and guide model designs. The contributions
                of our work vary from augmenting research in the field of meta
                learning to explaining the success of the method through
                geometrical lens.
              </p>
            </div>
          </div>
        </div>

        <!-- dissecting adv -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingRobDissect">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseRobDissect"
              aria-expanded="false"
              aria-controls="flush-collapseRobDissect"
            >
              Dissecting Robust Neural Networks (2019)
            </button>
          </h2>
          <div
            id="flush-collapseRobDissect"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingRobDissect"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1APiihQP8XTeIGF8ZP86vd6Q_vBNShy-b/view?usp=sharing"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >
              |
              <a
                href="https://docs.google.com/presentation/d/e/2PACX-1vSqaVm_AEvDVfqOrQzLullmEpe8HRYDO1N02qTm2B_XKIIAdE0IPEykJHUClIBOVrYG-xUJle586L-_/pub?start=false&loop=false&delayms=3000"
                target="_blank"
                rel="noopener noreferrer"
                >Slides</a
              >
              <img
                src="img/rob_dissect.png"
                alt="robust dissection figure"
                style="max-width: 60%; margin-left: 20px"
              />
              <p>
                We explore the hypothesis that robust training helps to
                disentangle representations by looking at the behavior of
                individual units of a robustly trained neural network through
                network dissection. We observe that to a large extent, robust
                models explain each high-level concept with fewer units than the
                equivalent non-robust models.
              </p>
            </div>
          </div>
        </div>

        <!-- karger stein -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingThree">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseThree"
              aria-expanded="false"
              aria-controls="flush-collapseThree"
              style="text-align: left"
            >
              Survey of the Recent Progress on the Runtime of the Karger-Stein
              Algorithm (2020)
            </button>
          </h2>
          <div
            id="flush-collapseThree"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingThree"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1leRazdwKrOKU9iWuOcEubFKO0JbHqvGB/view?usp=sharing"
                target="_blank"
                rel="noopener noreferrer"
                >Paper</a
              >

              <figure class="figure">
                <img
                  src="img/karger_stein.png"
                  class="figure-img img-fluid rounded"
                  alt="Variationally coherent paper figure"
                  style="max-width: 98%"
                />
                <figcaption class="figure-caption">
                  A successfull run of the Karger-Stein contraction algorithm
                  [Karger's Algorithm Wikipedia Entry]
                </figcaption>
              </figure>

              <p>
                The minimum \(k\)-cut problem is a natural generalization of the
                famous minimum cut problem, where we seek a cut that partitions
                a graph \(G(V,E)\) into \(k\) components. Karger et al. (1996)
                develop a randomized algorithm and provide an analysis which
                shows their algorithm provides a solution with high probability
                in \(\tilde{\mathcal{O}}\left( |V|^{2(k-1)} \right)\) time. A
                reduction to the maximum-weight clique problem gives a lower
                bound of \(\mathcal{\Omega}\left( |V|^{(1-o(1))k} \right)\).
                Until recently, the gap in the exponent was not explained.
                Confirming previous experimental results, Anuram Gupta et al.
                (2020) provide a refined analysis of the Karger-Stein algorithm
                that shows it is tight up to lower order terms. We survey the
                techniques employed in their analysis, together with other
                recent analyses of algorithms of similar flavour; this includes
                ideas from extremal set theory and stochastic processes.
              </p>
              <p class="text-muted">
                Anupam Gupta et al. “Optimal Bounds for the k-cut Problem”. In:
                arXiv preprint arXiv:2005.08301 (2020).
              </p>
              <p class="text-muted">
                David R. Karger and Clifford Stein. “A New Approach to the
                Minimum Cut Problem”. In: J. ACM 43.4 (July 1996), pp. 601–640.
              </p>
              <p class="text-muted">
                Wikipedia, Karger's Algorithm
                https://en.wikipedia.org/wiki/Karger%27s_algorithm
              </p>
            </div>
          </div>
        </div>

        <!-- feynman-kac -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingFeynmanKac">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseFeynmanKac"
              aria-expanded="false"
              aria-controls="flush-collapseFeynmanKac"
              style="text-align: left"
            >
              Monte Carlo Methods for P(I)DEs: Feynman-Kac for Jump-Diffusions
              (2019)
            </button>
          </h2>
          <div
            id="flush-collapseFeynmanKac"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingFeynmanKac"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1BHSAqCiea69z0sLim41cKOwHuINt9YHH/view?usp=sharing"
                target="_blank"
                >Paper
              </a>
              <img
                src="img/kde.png"
                alt="Feynman-Kac project"
                style="max-width: 60%; margin-left: 20px"
              />
              <p>
                Continuous time Markov processes are heavily used to approximate
                solutions to parabolic and elliptic partial differential
                equations. In this paper, we examine the role of the Feynman-Kac
                formula in the aforementioned approximation. We present the
                classical results for linear parabolic equations. Then we
                generalize the method for stochastic processes with discrete
                jump discontinuities. Via Fokker-Planck's equation we show that
                this class of SDEs is the codomain of a more general class of
                partial integro-differential equations under Feynman-Kac's
                transformation. We utilize a jump-adapted Euler-Maruyama scheme
                to approximate the evolution of jump-diffusions. Finally, we
                analyze the weak and strong convergence properties of the
                implemented methods.
              </p>
            </div>
          </div>
        </div>

        <!-- RSI -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingRSI">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseRSI"
              aria-expanded="false"
              aria-controls="flush-collapseRSI"
            >
              On the Size of Unions of Lines Satisfying the Wolff Axiom
              (Research Science Institute 2015)
            </button>
          </h2>
          <div
            id="flush-collapseRSI"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingRSI"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              (In Proceedings of the Forty-sixth Spring Conference of the Union
              of Bulgarian Mathematicians 2017.) <br />
              <a
                href="https://www.cee.org/news/eighty-four-high-school-students-selected-and-participating-32nd-research-science-institute-mit"
                target="_blank"
                >Paper
              </a>
              <img
                class="activator"
                src="img/besicovitch.jpeg"
                style="max-width: 60%; margin-left: 20px"
                alt="besicovitch-set"
              />
              <br />
              The paper deals with finding unions of lines in
              \(\mathbb{F}^n_{p^{2k}}\) which obey the Wolff axiom and have
              minimal size. We provide an extension of the constructions for
              \(\mathbb{F}^3_{p^{2k}}\), obtained by Tao in 2002, to a
              construction for \(\mathbb{F}^n_{p^{2k}}\). We determine the size
              of the union of lines in our construction in
              \(\mathbb{F}^n_{p^{2k}}\) to be
              \(\mathcal{O}\left(p^{1.6kn}\right)\). We prove that our
              construction obeys the Wolff axiom up to a heuristically
              negligible number of lines.
            </div>
          </div>
        </div>

        <!-- 2n-gons -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingOne">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseOne"
              aria-expanded="false"
              aria-controls="flush-collapseOne"
              style="text-align: left"
            >
              On the Number of Two-Dimensional Manifolds Obtained by Gluing the
              Edges of Polygons (2014)
            </button>
          </h2>
          <div
            id="flush-collapseOne"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingOne"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a
                href="https://drive.google.com/file/d/1ji1EQ2Bp_3ka4Mm-K0LC6Z-ck76UmKBs/view?usp=sharing"
                target="_blank"
                >Paper</a
              >
              <img
                class="activator"
                src="img/add-sphere.png"
                style="max-width: 60%; margin-left: 20px"
                alt="glue-a-sphere-to-a-torus"
              />
              <p>
                The project belongs to the field of topology. More specifically,
                the identification (gluing) of the edges of \(2n\)-gons is
                analyzed. We prove that the resulting manifold is a topological
                surface. A classification of compact surfaces is presented. A
                conjecture regarding the number of ways to obtain an arbitrary
                manifold from \(2n\)-gon is constructed. The author’s
                contributions consist of constructing a conjecture regarding the
                number of ways to obtain an arbitrary manifold from \(2n\)-gon,
                as well as computing the number of configurations of the edges
                of a \(2n\)-gon forming a sphere, a torus, a \(\mathbb{R}P^2\)
                and a Klein bottle, thus proving the conjecture true for the
                mentioned manifolds.
              </p>
              <p>
                I started developing this project in the summer of 2014 at
                <a
                  href="http://www.math.bas.bg/omi/hssimi/?lang=en"
                  target="_blank"
                  rel="noopener noreferrer"
                  >HSSIMI</a
                >
                - a summer research school held at Blagoevgrad, Bulgaria. I
                worked under the guidance of Katerina Velcheva from Stanford.
                Afterwards, I continued working on the topic under the guidance
                of Prof. Kopanov from Plovdiv University. The project was
                presented at the 27th edition of
                <a
                  href="http://www.eucys2015.eu/"
                  target="_blank"
                  rel="noopener noreferrer"
                  >EUCYS</a
                >, the largest science fair for high school students in Europe.
              </p>
            </div>
          </div>
        </div>

        <!-- foosrank -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingTwo">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#flush-collapseTwo"
              aria-expanded="false"
              aria-controls="flush-collapseTwo"
            >
              FoosRank (HackMIT 2018)
            </button>
          </h2>
          <div
            id="flush-collapseTwo"
            class="accordion-collapse collapse"
            aria-labelledby="flush-headingTwo"
            data-bs-parent="#accordionFlushProjects"
          >
            <div class="accordion-body">
              <a href="https://foosrank-9e12f.firebaseapp.com/" target="_blank">
                Web app
              </a>
              |
              <a
                href="https://github.com/kristian-georgiev/FoosRank"
                target="_blank"
                >Repo</a
              >
              <img
                src="img/foosrank.png"
                alt="foosrank logo"
                style="max-width: 50%"
                class="activator"
              />
              <p>
                In college, I played lots of foosball. And so did a lot of my
                friends. Being a competitive bunch, we created FoosRank - a
                ranking system for foosball games. The algorithm is a slight
                modification of the ELO system, used in, e.g., chess. FoosRank
                also keeps the score during the game and shows each user their
                game history and ranking over time.
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- Teaching section -->
      <h2 style="margin-top: 40px">Teaching</h2>
      <ul class="list-group list-group-flush">
        <li class="list-group-item align-items-center">
          Introduction to Deep Learning (<a
            href="http://introtodeeplearning.com/"
            target="_blank"
            rel="noopener noreferrer"
            >6.S191</a
          >)
          <span class="badge bg-danger">teaching assistant</span>
          <span class="badge bg-light text-dark">IAP (Winter) 2020</span>
          <span class="badge bg-light text-dark">IAP (Winter) 2021</span>
        </li>
        <li class="list-group-item align-items-center">
          Introduction to Inference (6.008)
          <span class="badge bg-warning text-dark">lab assistant</span>
          <span class="badge bg-light text-dark">Fall 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Introduction to Machine Learning (6.036)
          <span class="badge bg-warning text-dark">lab assistant</span>
          <span class="badge bg-light text-dark">Spring 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Abstract Algebra I (18.701)
          <span class="badge bg-info text-dark">grader</span>
          <span class="badge bg-light text-dark">Fall 2018</span>
        </li>
      </ul>


      <!-- Classes section -->
      <!-- <h2 style="margin-top: 40px">Coursework</h2>
      <p>Some of my favorite classes.</p>
      <ul class="list-group list-group-flush">
        <li class="list-group-item align-items-center">
          Machine Learning (6.867)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Fall 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Optimization for Machine Learning (<a
            href="http://optml.mit.edu/teach/6881/"
            target="_blank"
            rel="noopener noreferrer"
            >6.881</a
          >)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Spring 2020</span>
        </li>
        <li class="list-group-item align-items-center">
          Topics in Deployable Machine Learning (<a
            href="https://people.csail.mit.edu/madry/6.S979/"
            target="_blank"
            rel="noopener noreferrer"
            >6.S979</a
          >)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Fall 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Fundamentals of Probability (<a
            href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2018/"
            target="_blank"
            rel="noopener noreferrer"
            >6.436</a
          >)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Fall 2018</span>
        </li>
        <li class="list-group-item align-items-center">
          Mathematical Statistics (<a
            href="https://www.mit.edu/~18.655/"
            target="_blank"
            rel="noopener noreferrer"
            >18.655</a
          >)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Spring 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Advanced Algorithms (<a
            href="http://courses.csail.mit.edu/6.854/20/"
            target="_blank"
            rel="noopener noreferrer"
            >6.854</a
          >)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Fall 2020</span>
        </li>
        <li class="list-group-item align-items-center">
          Modern Discrete Probability and Stochastic Processes (6.265)
          <span class="badge bg-primary">Graduate</span>
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Spring 2020</span>
        </li>
        <li class="list-group-item align-items-center">
          Abstract Algebra I (18.701)
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Fall 2017</span>
        </li>
        <li class="list-group-item align-items-center">
          Linear PDEs: Analysis and Numerics (<a
            href="https://github.com/mitmath/18303"
            target="_blank"
            rel="noopener noreferrer"
            >18.303</a
          >)
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Spring 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Introduction to Functional Analysis (<a
            href="https://math.mit.edu/~rbm/18.102Sp20/18.102Sp20.html"
            target="_blank"
            rel="noopener noreferrer"
            >18.102</a
          >)
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Spring 2020</span>
        </li>
        <li class="list-group-item align-items-center">
          Combinatorial Analysis (<a
            href="https://yufeizhao.com/211/"
            target="_blank"
            rel="noopener noreferrer"
            >18.211</a
          >)
          <span class="badge bg-success">Math</span>
          <span class="badge bg-light text-dark">Fall 2018</span>
        </li>
        <li class="list-group-item align-items-center">
          Elements of Software Construction (<a
            href="http://web.mit.edu/6.031/www/fa19/"
            target="_blank"
            rel="noopener noreferrer"
            >6.031</a
          >)
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Fall 2019</span>
        </li>
        <li class="list-group-item align-items-center">
          Design and Analysis of Algorithms (6.046)
          <span class="badge bg-secondary">CS</span>
          <span class="badge bg-light text-dark">Fall 2019</span>
        </li>
      </ul>
    </div> -->
    <!-- end of container -->

    <!--  Scripts-->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js"
      iintegrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW"
      crossorigin="anonymous"
    ></script>
    <!-- Google Analytics -->
    <script>
      (function (i, s, o, g, r, a, m) {
        i["GoogleAnalyticsObject"] = r;
        (i[r] =
          i[r] ||
          function () {
            (i[r].q = i[r].q || []).push(arguments);
          }),
          (i[r].l = 1 * new Date());
        (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
      })(
        window,
        document,
        "script",
        "https://www.google-analytics.com/analytics.js",
        "ga"
      );
      ga("create", "UA-XXXXX-Y", "auto");
      ga("send", "pageview");
    </script>
    <!-- End Google Analytics -->
  </body>
</html>